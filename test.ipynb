{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e72f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import psutil\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, MapType\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from pyspark import SparkContext\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ecefca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing costum module , don't need both but i have both lol \n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"./src/\")))\n",
    "\n",
    "ROOT_DIR = \".\"\n",
    "SRC_DIR = ROOT_DIR + \"/src\"\n",
    "sys.path.append(\"/Users/admin/projectst/EEG-Feature-Benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6376fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    " if SparkContext._active_spark_context:\n",
    "        print(\"Stopping existing Spark context...\")\n",
    "        SparkContext._active_spark_context.stop()\n",
    "        print(\"Previous Spark context stopped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356f6544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "25/05/04 20:31:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/04 20:31:15 WARN Utils: Service 'SparkUI' could not bind on port 57690. Attempting port 57691.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"EEG_Analysis_HPC\")\n",
    "\n",
    "        # Target ~12 executors across the cluster (safe parallelism)\n",
    "        .config(\"spark.executor.instances\", \"12\")\n",
    "        .config(\"spark.executor.cores\", \"1\")             # 1 core per executor\n",
    "        .config(\"spark.executor.memory\", \"4g\")           # 12 Ã— 4g = 48 GB total\n",
    "        .config(\"spark.driver.memory\", \"6g\")             # leave room for driver ops\n",
    "\n",
    "        # Parallelism and shuffle tuning\n",
    "        .config(\"spark.default.parallelism\", \"12\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"12\")\n",
    "\n",
    "        # UDF / pandas optimization\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"500\")\n",
    "\n",
    "        # JVM GC tuning\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:+UseStringDeduplication\")\n",
    "\n",
    "        # Optional: Raise broadcast size if EEG config or schema is large\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"20MB\")\n",
    "\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c720dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Spark session created successfully\n",
      "=== Spark Configuration & Runtime info ===\n",
      "App ID: app-20250504203115-0002\n",
      "Master: spark://cm001:23412\n",
      "Default Parallelism: 12\n",
      "Total Executors: 1\n",
      "spark.default.parallelism = 12\n",
      "spark.sql.execution.arrow.maxRecordsPerBatch = 500\n",
      "spark.executor.instances = 12\n",
      "spark.executor.extraJavaOptions = -XX:+UseG1GC -XX:+UseStringDeduplication\n",
      "spark.sql.shuffle.partitions = 12\n",
      "spark.ui.killEnabled = false\n",
      "spark.driver.port = 45357\n",
      "spark.driver.memory = 6g\n",
      "spark.ui.proxyBase = /rnode/cm001.hpc.nyu.edu/57690\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.master = spark://cm001:23412\n",
      "spark.submit.deployMode = client\n",
      "spark.app.startTime = 1746405074396\n",
      "spark.sql.execution.arrow.pyspark.enabled = true\n",
      "spark.driver.host = cm001.hpc.nyu.edu\n",
      "spark.executor.memory = 4g\n",
      "spark.ui.enabled = true\n",
      "spark.sql.warehouse.dir = file:/home/ans9868/EEG-HPC-benchmark/spark-warehouse\n",
      "spark.executor.id = driver\n",
      "spark.app.id = app-20250504203115-0002\n",
      "spark.sql.autoBroadcastJoinThreshold = 20MB\n",
      "spark.executorEnv.JAVA_HOME = /usr/lib/jvm/java-11-openjdk-amd64\n",
      "spark.ui.port = 57690\n",
      "spark.rdd.compress = True\n",
      "spark.app.name = EEG_Analysis_HPC\n",
      "spark.submit.pyFiles = \n",
      "spark.executor.cores = 1\n",
      "spark.ui.showConsoleProgress = true\n"
     ]
    }
   ],
   "source": [
    "print(\"New Spark session created successfully\")\n",
    "print(\"=== Spark Configuration & Runtime info ===\")\n",
    "print(f\"App ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Total Executors: {spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size()}\")\n",
    "\n",
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{item[0]} = {item[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429fc9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
