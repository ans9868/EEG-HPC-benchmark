{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657937d6-eae5-4dcf-8a03-0c0f40bfe5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e72f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import psutil\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, MapType\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from pyspark import SparkContext\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356f6544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/05 20:15:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/05 20:15:35 WARN SparkSession: Cannot use ndb.NDBSparkSessionExtension to configure session extensions.\n",
      "java.lang.ClassNotFoundException: ndb.NDBSparkSessionExtension\n",
      "\tat java.base/java.lang.ClassLoader.findClass(ClassLoader.java:724)\n",
      "\tat org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n",
      "\tat org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)\n",
      "\tat org.apache.spark.util.ChildFirstURLClassLoader.loadClass(ChildFirstURLClassLoader.java:57)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:926)\n",
      "\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)\n",
      "\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"EEG_Analysis_HPC\")\n",
    "\n",
    "        # Target ~12 executors across the cluster (safe parallelism)\n",
    "        .config(\"spark.executor.instances\", \"12\")\n",
    "        .config(\"spark.executor.cores\", \"1\")             # 1 core per executor\n",
    "        .config(\"spark.executor.memory\", \"4g\")           # 12 × 4g = 48 GB total\n",
    "        .config(\"spark.driver.memory\", \"6g\")             # leave room for driver ops\n",
    "\n",
    "        # Parallelism and shuffle tuning\n",
    "        .config(\"spark.default.parallelism\", \"12\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"12\")\n",
    "\n",
    "        # UDF / pandas optimization\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"500\")\n",
    "\n",
    "        # JVM GC tuning\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:+UseStringDeduplication\")\n",
    "\n",
    "        # Optional: Raise broadcast size if EEG config or schema is large\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"20MB\")\n",
    "\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c720dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Spark session created successfully\n",
      "=== Spark Configuration & Runtime info ===\n",
      "App ID: app-20250505201534-0000\n",
      "Master: spark://cm012:38290\n",
      "Default Parallelism: 12\n",
      "Total Executors: 17\n",
      "spark.driver.port = 35363\n",
      "spark.default.parallelism = 12\n",
      "spark.shuffle.io.connectionTimeout = 3000s\n",
      "spark.sql.shuffle.partitions = 12\n",
      "spark.ui.killEnabled = false\n",
      "spark.ui.enabled = true\n",
      "spark.driver.userClassPathFirst = true\n",
      "spark.app.startTime = 1746490533383\n",
      "spark.ndb.parallel_import = true\n",
      "spark.ndb.access_key_id = \n",
      "spark.app.submitTime = 1746490533183\n",
      "spark.ui.proxyBase = /rnode/cm012.hpc.nyu.edu/56849\n",
      "spark.executor.userClassPathFirst = true\n",
      "spark.rdd.compress = True\n",
      "spark.executorEnv.JAVA_HOME = /usr/lib/jvm/java-11-openjdk-amd64\n",
      "spark.master = spark://cm012:38290\n",
      "spark.sql.catalog.ndb = spark.sql.catalog.ndb.VastCatalog\n",
      "spark.ndb.retry_max_count = 3\n",
      "spark.ndb.secret_access_key = \n",
      "spark.sql.execution.arrow.pyspark.enabled = true\n",
      "spark.ndb.num_of_splits = 8\n",
      "spark.rpc.io.connectionTimeout = 3000s\n",
      "spark.ndb.retry_sleep_duration = 1\n",
      "spark.ndb.dynamic_filtering_wait_timeout = 2\n",
      "spark.ndb.dynamic_filter_compaction_threshold = 100\n",
      "spark.network.timeout = 3600s\n",
      "spark.ui.port = 56849\n",
      "spark.executor.memory = 4g\n",
      "spark.python.authenticate.socketTimeout = 1m\n",
      "spark.task.reaper.pollingInterval = 3000s\n",
      "spark.executor.heartbeatInterval = 3000s\n",
      "spark.ndb.num_of_sub_splits = 8\n",
      "spark.executor.id = driver\n",
      "spark.submit.deployMode = client\n",
      "spark.ndb.endpoint = http://10.32.38.210\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.sql.extensions = ndb.NDBSparkSessionExtension\n",
      "spark.driver.memory = 6g\n",
      "spark.ui.showConsoleProgress = true\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC -XX:+UseStringDeduplication\n",
      "spark.sql.execution.arrow.maxRecordsPerBatch = 500\n",
      "spark.executor.cores = 1\n",
      "spark.task.maxFailures = 1\n",
      "spark.driver.host = cm012.hpc.nyu.edu\n",
      "spark.shuffle.push.finalize.timeout = 3000s\n",
      "spark.port.maxRetries = 30\n",
      "spark.ndb.rowgroups_per_subsplit = 1\n",
      "spark.app.id = app-20250505201534-0000\n",
      "spark.ndb.query_data_rows_per_split = 4000000\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.submit.pyFiles = \n",
      "spark.app.name = EEG_Analysis_HPC\n",
      "spark.ndb.data_endpoints = http://10.32.38.210,http://10.32.38.211,http://10.32.38.212,http://10.32.38.213,http://10.32.38.214,http://10.32.38.215,http://10.32.38.216,http://10.32.38.217\n",
      "spark.rpc.askTimeout = 3000s\n",
      "spark.executor.instances = 12\n",
      "spark.sql.autoBroadcastJoinThreshold = 20MB\n"
     ]
    }
   ],
   "source": [
    "print(\"New Spark session created successfully\")\n",
    "print(\"=== Spark Configuration & Runtime info ===\")\n",
    "print(f\"App ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Total Executors: {spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size()}\")\n",
    "\n",
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{item[0]} = {item[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429fc9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 20:15:48 WARN Dispatcher: Message RemoteProcessDisconnected(10.32.35.26:37420) dropped. Could not find BlockManagerEndpoint1.\n",
      "25/05/05 20:15:48 WARN Dispatcher: Message RemoteProcessDisconnected(10.32.35.26:37402) dropped. Could not find BlockManagerEndpoint1.\n",
      "25/05/05 20:15:48 WARN Dispatcher: Message RemoteProcessDisconnected(10.32.35.26:37460) dropped. Could not find BlockManagerEndpoint1.\n",
      "25/05/05 20:15:48 WARN Dispatcher: Message RemoteProcessDisconnected(10.32.35.25:33088) dropped. Could not find BlockManagerEndpoint1.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"EEG_Analysis_HPC\")\n",
    "    .config(\"spark.executor.cores\", \"5\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"EEG_Analysis_HPC\")\n",
    "\n",
    "        # Target ~12 executors across the cluster (safe parallelism)\n",
    "        .config(\"spark.executor.instances\", \"12\")\n",
    "        .config(\"spark.executor.cores\", \"1\")             # 1 core per executor\n",
    "        .config(\"spark.executor.memory\", \"4g\")           # 12 × 4g = 48 GB total\n",
    "        .config(\"spark.driver.memory\", \"6g\")             # leave room for driver ops\n",
    "\n",
    "        # Parallelism and shuffle tuning\n",
    "        .config(\"spark.default.parallelism\", \"12\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"12\")\n",
    "\n",
    "        # UDF / pandas optimization\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"500\")\n",
    "\n",
    "        # JVM GC tuning\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:+UseStringDeduplication\")\n",
    "\n",
    "        # Optional: Raise broadcast size if EEG config or schema is large\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"20MB\")\n",
    "\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
