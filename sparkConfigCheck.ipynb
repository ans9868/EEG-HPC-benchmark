{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc6b76b-d683-4f8c-aef2-a2d905845b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5b4561-6b7e-4110-9de8-1e6625b8fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import psutil\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, MapType\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from pyspark import SparkContext\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b79d604-c2b2-4768-a7fe-abb41af8228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster configuration for PySpark - 2 nodes (16 cores, 64GB RAM total)\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Define storage paths (modify as needed for your cluster)\n",
    "external_ssd_path = \"/scratch/ans9868/spark_temp\"\n",
    "os.makedirs(external_ssd_path, exist_ok=True)\n",
    "\n",
    "# Calculate cluster resources\n",
    "total_cores = 16\n",
    "total_memory_gb = 64\n",
    "nodes = 2\n",
    "\n",
    "# Reserve memory for OS and system processes (15% per node)\n",
    "reserve_memory_per_node = total_memory_gb / nodes * 0.15\n",
    "available_memory_gb = total_memory_gb - (reserve_memory_per_node * nodes)\n",
    "\n",
    "# Allocate resources\n",
    "cores_per_executor = 2  # Recommended size for good parallelism without excessive overhead\n",
    "num_executors = nodes * (total_cores // cores_per_executor) // nodes - 1  # Reserve 1 core per node for overhead\n",
    "\n",
    "# Memory settings (account for overhead)\n",
    "executor_memory_gb = int((available_memory_gb * 0.8) / num_executors)  # 80% for executors\n",
    "driver_memory_gb = int(available_memory_gb * 0.2)  # 20% for driver\n",
    "\n",
    "# Calculate executor overhead (roughly 10% of executor memory)\n",
    "executor_overhead_mb = int(executor_memory_gb * 1024 * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ade696-0df1-42e7-83af-d2918941ced7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster configuration:\n",
      "  - Total nodes: 2\n",
      "  - Total cores: 16\n",
      "  - Total memory: 64GB\n",
      "  - Number of executors: 7\n",
      "  - Cores per executor: 2\n",
      "  - Driver memory: 10GB\n",
      "  - Executor memory: 6GB\n",
      "  - Executor overhead: 614MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/08 21:42:06 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/05/08 21:42:06 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "25/05/08 21:42:07 WARN SparkSession: Cannot use ndb.NDBSparkSessionExtension to configure session extensions.\n",
      "java.lang.ClassNotFoundException: ndb.NDBSparkSessionExtension\n",
      "\tat java.base/java.lang.ClassLoader.findClass(ClassLoader.java:724)\n",
      "\tat org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n",
      "\tat org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)\n",
      "\tat org.apache.spark.util.ChildFirstURLClassLoader.loadClass(ChildFirstURLClassLoader.java:57)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:926)\n",
      "\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)\n",
      "\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:105)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Active Spark Configuration:\n",
      "Default Parallelism: 32\n",
      "Shuffle Partitions: 64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Cluster configuration:\")\n",
    "print(f\"  - Total nodes: {nodes}\")\n",
    "print(f\"  - Total cores: {total_cores}\")\n",
    "print(f\"  - Total memory: {total_memory_gb}GB\")\n",
    "print(f\"  - Number of executors: {num_executors}\")\n",
    "print(f\"  - Cores per executor: {cores_per_executor}\")\n",
    "print(f\"  - Driver memory: {driver_memory_gb}GB\")\n",
    "print(f\"  - Executor memory: {executor_memory_gb}GB\")\n",
    "print(f\"  - Executor overhead: {executor_overhead_mb}MB\")\n",
    "\n",
    "# Create SparkSession with distributed cluster configuration\n",
    "master = \"spark://cm045:14089\"\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"EEG-Analysis-Cluster\")\n",
    "    # .master(master)  # Set to your cluster's master URL\n",
    "    \n",
    "    # Resource allocation\n",
    "    .config(\"spark.executor.instances\", str(num_executors))\n",
    "    .config(\"spark.executor.cores\", str(cores_per_executor))\n",
    "    .config(\"spark.executor.memory\", f\"{executor_memory_gb}g\")\n",
    "    .config(\"spark.driver.memory\", f\"{driver_memory_gb}g\")\n",
    "    .config(\"spark.executor.memoryOverhead\", f\"{executor_overhead_mb}m\")\n",
    "    \n",
    "    # Performance tuning\n",
    "    .config(\"spark.default.parallelism\", str(total_cores * 2))  # 2x total cores\n",
    "    .config(\"spark.sql.shuffle.partitions\", str(total_cores * 4))  # 4x total cores\n",
    "    .config(\"spark.memory.fraction\", \"0.75\")  # Higher for data processing workloads\n",
    "    .config(\"spark.memory.storageFraction\", \"0.4\")  # Balanced for compute/storage\n",
    "    .config(\"spark.driver.maxResultSize\", f\"{driver_memory_gb // 2}g\")\n",
    "    \n",
    "    # Network and shuffle optimizations\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"96m\")  # Larger for faster network transfers\n",
    "    .config(\"spark.shuffle.file.buffer\", \"2m\")  # Increased for better I/O\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")  # More resilient in distributed environment\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"30s\")  # Wait longer between retries\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"10s\")  # Set a much shorter heartbeat interval\n",
    "    .config(\"spark.network.timeout\", \"800s\")  # Prevent timeouts on larger operations\n",
    "    \n",
    "    # Storage locations\n",
    "    .config(\"spark.local.dir\", external_ssd_path)\n",
    "    .config(\"spark.worker.dir\", external_ssd_path)\n",
    "    .config(\"spark.sql.warehouse.dir\", f\"{external_ssd_path}/warehouse\")\n",
    "    \n",
    "    # Memory management and GC\n",
    "    .config(\"spark.driver.extraJavaOptions\", \n",
    "            f\"-Djava.io.tmpdir={external_ssd_path}/tmp -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+HeapDumpOnOutOfMemoryError\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", \n",
    "            f\"-Djava.io.tmpdir={external_ssd_path}/tmp -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark\")\n",
    "    \n",
    "    # Data compression - snappy is balanced for speed/compression ratio\n",
    "    .config(\"spark.io.compression.codec\", \"snappy\")\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "    .config(\"spark.broadcast.compress\", \"true\")\n",
    "    \n",
    "    # Dynamic allocation (optional, remove if you want fixed allocation)\n",
    "    #.config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    #.config(\"spark.dynamicAllocation.minExecutors\", str(num_executors // 2))\n",
    "    #.config(\"spark.dynamicAllocation.maxExecutors\", str(num_executors + 4))\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Optional: Set log level to reduce console output\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Display actual configuration for verification\n",
    "print(\"\\nActive Spark Configuration:\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc205e12-0f9d-4185-a0dd-060b276c3b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Spark session created successfully\n",
      "=== Spark Configuration & Runtime info ===\n",
      "App ID: app-20250508214207-0000\n",
      "Master: spark://cm045:50174\n",
      "Default Parallelism: 32\n",
      "Total Executors: 9\n",
      "spark.shuffle.io.connectionTimeout = 3000s\n",
      "spark.ui.killEnabled = false\n",
      "spark.ui.enabled = true\n",
      "spark.driver.userClassPathFirst = true\n",
      "spark.app.submitTime = 1746754810195\n",
      "spark.ndb.parallel_import = true\n",
      "spark.ndb.access_key_id = \n",
      "spark.executor.cores = 2\n",
      "spark.shuffle.io.maxRetries = 10\n",
      "spark.executor.userClassPathFirst = true\n",
      "spark.driver.memory = 10g\n",
      "spark.shuffle.compress = true\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.io.tmpdir=/scratch/ans9868/spark_temp/tmp -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -XX:+HeapDumpOnOutOfMemoryError\n",
      "spark.reducer.maxSizeInFlight = 96m\n",
      "spark.rdd.compress = True\n",
      "spark.driver.port = 33705\n",
      "spark.executorEnv.JAVA_HOME = /usr/lib/jvm/java-11-openjdk-amd64\n",
      "spark.app.name = EEG-Analysis-Cluster\n",
      "spark.sql.catalog.ndb = spark.sql.catalog.ndb.VastCatalog\n",
      "spark.ndb.retry_max_count = 3\n",
      "spark.ndb.secret_access_key = \n",
      "spark.sql.execution.arrow.pyspark.enabled = true\n",
      "spark.ndb.num_of_splits = 8\n",
      "spark.network.timeout = 800s\n",
      "spark.ui.port = 4573\n",
      "spark.memory.storageFraction = 0.4\n",
      "spark.worker.dir = /scratch/ans9868/spark_temp\n",
      "spark.ui.proxyBase = /rnode/cm045.hpc.nyu.edu/4573\n",
      "spark.rpc.io.connectionTimeout = 3000s\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.io.tmpdir=/scratch/ans9868/spark_temp/tmp -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark\n",
      "spark.ndb.retry_sleep_duration = 1\n",
      "spark.ndb.dynamic_filtering_wait_timeout = 2\n",
      "spark.ndb.dynamic_filter_compaction_threshold = 100\n",
      "spark.app.id = app-20250508214207-0000\n",
      "spark.io.compression.codec = snappy\n",
      "spark.shuffle.file.buffer = 2m\n",
      "spark.local.dir = /scratch/ans9868/spark_temp\n",
      "spark.python.authenticate.socketTimeout = 1m\n",
      "spark.task.reaper.pollingInterval = 3000s\n",
      "spark.executor.memory = 6g\n",
      "spark.ndb.num_of_sub_splits = 8\n",
      "spark.executor.memoryOverhead = 614m\n",
      "spark.driver.maxResultSize = 5g\n",
      "spark.executor.id = driver\n",
      "spark.sql.shuffle.partitions = 64\n",
      "spark.app.startTime = 1746754926287\n",
      "spark.shuffle.io.retryWait = 30s\n",
      "spark.submit.deployMode = client\n",
      "spark.sql.warehouse.dir = file:/scratch/ans9868/spark_temp/warehouse\n",
      "spark.ndb.endpoint = http://10.32.38.210\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.broadcast.compress = true\n",
      "spark.sql.extensions = ndb.NDBSparkSessionExtension\n",
      "spark.ui.showConsoleProgress = true\n",
      "spark.executor.heartbeatInterval = 10s\n",
      "spark.task.maxFailures = 1\n",
      "spark.memory.fraction = 0.75\n",
      "spark.shuffle.push.finalize.timeout = 3000s\n",
      "spark.shuffle.spill.compress = true\n",
      "spark.port.maxRetries = 30\n",
      "spark.ndb.rowgroups_per_subsplit = 1\n",
      "spark.master = spark://cm045:50174\n",
      "spark.driver.host = cm045\n",
      "spark.ndb.query_data_rows_per_split = 4000000\n",
      "spark.executor.instances = 7\n",
      "spark.submit.pyFiles = \n",
      "spark.ndb.data_endpoints = http://10.32.38.210,http://10.32.38.211,http://10.32.38.212,http://10.32.38.213,http://10.32.38.214,http://10.32.38.215,http://10.32.38.216,http://10.32.38.217\n",
      "spark.rpc.askTimeout = 3000s\n",
      "spark.default.parallelism = 32\n"
     ]
    }
   ],
   "source": [
    "print(\"New Spark session created successfully\")\n",
    "print(\"=== Spark Configuration & Runtime info ===\")\n",
    "print(f\"App ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Total Executors: {spark.sparkContext._jsc.sc().getExecutorMemoryStatus().size()}\")\n",
    "\n",
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{item[0]} = {item[1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d935fc-393d-41ab-94ff-91c87371834d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37748824-bc13-499e-b8e1-4065aa19f5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e5fab-751f-4a57-9113-0c97118e63b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b15d0-5d06-44c7-8df8-21e87a1208fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb5bdd-a06f-42ae-8ae6-958969186602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698f26b7-36f4-435b-992a-8901e595dd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c1576-e2bb-4478-b371-8915c3fc996a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6180b0-cc1c-408b-a1fd-cac5264e91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e29c8e-6970-4aa7-bbb5-4560bbcd9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_ssd_path = \"/scratch/ans9868/spark_temp\"\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"EEG-Analysis\")\n",
    "\n",
    "    # Master should be set if you're not using spark-submit with --master\n",
    "    # Memory and cores per executor\n",
    "    .config(\"spark.executor.instances\", \"8\")             # Total executors (across both nodes)\n",
    "    .config(\"spark.executor.cores\", \"4\")                 # Cores per executor\n",
    "    .config(\"spark.executor.memory\", \"8g\")               # Memory per executor\n",
    "    .config(\"spark.driver.memory\", \"8g\")                 # Driver memory\n",
    "\n",
    "    # Memory tuning\n",
    "    .config(\"spark.memory.fraction\", \"0.7\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")          # Result limit to prevent OOM\n",
    "\n",
    "    # Shuffle & parallelism tuning\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"64\")        # = total cores (2 nodes × 16)\n",
    "    .config(\"spark.default.parallelism\", \"64\")\n",
    "    .config(\"spark.shuffle.file.buffer\", \"1m\")\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.spill.numElementsForceSpillThreshold\", \"5000\")\n",
    "\n",
    "    # External SSD paths\n",
    "    # .config(\"spark.local.dir\", \"/mnt/data/spark-local\")  # Temp local storage\n",
    "    # .config(\"spark.worker.dir\", \"/mnt/data/spark-worker\")\n",
    "    # .config(\"spark.sql.warehouse.dir\", \"/mnt/data/warehouse\")\n",
    "\n",
    "    # Temp and GC tuning\n",
    "    # .config(\"spark.driver.extraJavaOptions\", \"-Djava.io.tmpdir=/mnt/data/tmp -XX:+UseG1GC\")\n",
    "    # .config(\"spark.executor.extraJavaOptions\", \"-Djava.io.tmpdir=/mnt/data/tmp -XX:+UseG1GC\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
